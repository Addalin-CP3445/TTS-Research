{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOq6gnVgTomZ"
      },
      "source": [
        "The code for this training pipeline is derived from ESPnet github page : https://espnet.github.io/espnet/notebook/ESPnetEZ/TTS/TTS_finetune_vctk_dump.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25o88hp7SvKj"
      },
      "source": [
        "# Intalling espnet, espnet model zoo and camel tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhkw4pBspQgn"
      },
      "source": [
        "Installing espnet and espnet model zoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dvv7frPK8VMY",
        "outputId": "dc56e6ce-3e62-40a8-fec0-67360484c335"
      },
      "outputs": [],
      "source": [
        "!pip install espnet espnet_model_zoo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "takAEvEEpowy"
      },
      "source": [
        "Installing Camel Tools for converting the Buckwalter transcription to standard Arabic script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voHvfMZU8j1V",
        "outputId": "e1922501-f46b-44d8-bef1-5af12668442c"
      },
      "outputs": [],
      "source": [
        "!pip install camel-tools --no-build-isolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awYVJZCCSjPk"
      },
      "source": [
        "# Downloading and Pre-Processing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOVlM_BMp6qC"
      },
      "source": [
        "Downloading and Extracting Arabic Speech Corpus Dataset\n",
        "\n",
        "Link to website: https://en.arabicspeechcorpus.com/\n",
        "\n",
        "Sometimes the dataset will show error on downloading multiple times continously from the url. But it will resolve when you try again after some time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r0WD1s1_Gad",
        "outputId": "24a56f53-dacb-448c-e705-ce830d50c022"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "\n",
        "# Define the URL and output path\n",
        "url = \"https://en.arabicspeechcorpus.com/arabic-speech-corpus.zip\"\n",
        "zip_path = \"/content/arabic-speech-corpus.zip\"\n",
        "extract_path = \"/content\"\n",
        "\n",
        "# Download the dataset\n",
        "print(\"Downloading dataset...\")\n",
        "urllib.request.urlretrieve(url, zip_path)\n",
        "print(\"Download complete.\")\n",
        "\n",
        "# Unzip the dataset\n",
        "print(\"Extracting files...\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction complete.\")\n",
        "\n",
        "# List extracted files\n",
        "os.listdir(extract_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oijxYVXq-z3"
      },
      "source": [
        "Cloning Speaker Embeddings (X-vector) generated from the Dataset.\n",
        "\n",
        "Link: https://github.com/Addalin-CP3445/speaker_embedding/tree/main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2ncUgWhc14U",
        "outputId": "3cff8b33-759c-46a1-bde0-83684bfbfc0a"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Addalin-CP3445/speaker_embedding.git\n",
        "%cp speaker_embedding/extract_spk_embedding.py extract_spk_embedding.py\n",
        "%cp speaker_embedding/train_speaker_embeddings -r train_speaker_embeddings\n",
        "%cp speaker_embedding/test_speaker_embeddings -r test_speaker_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiS6rle5vSx4"
      },
      "source": [
        "Renaming Audio files for Training and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EieaaE64B-4X",
        "outputId": "63d6b277-ba28-49c5-bbe2-e83b3467c511"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the directory where your wav files are stored\n",
        "wav_directory = '/content/arabic-speech-corpus/test set/wav'  # <-- update this path\n",
        "\n",
        "# Define the prefix to remove\n",
        "prefix = \"ARA NORM  \"\n",
        "\n",
        "# Iterate through all files in the directory\n",
        "for filename in os.listdir(wav_directory):\n",
        "    if filename.startswith(prefix):\n",
        "        # Remove the prefix\n",
        "        new_filename = filename[len(prefix):]\n",
        "        old_filepath = os.path.join(wav_directory, filename)\n",
        "        new_filepath = os.path.join(wav_directory, new_filename)\n",
        "        print(f\"Renaming: {old_filepath} -> {new_filepath}\")\n",
        "        os.rename(old_filepath, new_filepath)\n",
        "\n",
        "print(\"Renaming testing audio files complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ja1QX4EByws",
        "outputId": "f62335a4-53ee-487f-fec7-aea7e15584df"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the directory where your wav files are stored\n",
        "wav_directory = '/content/arabic-speech-corpus/wav'  # <-- update this path\n",
        "\n",
        "# Define the prefix to remove\n",
        "prefix = \"ARA NORM  \"\n",
        "\n",
        "# Iterate through all files in the directory\n",
        "for filename in os.listdir(wav_directory):\n",
        "    if filename.startswith(prefix):\n",
        "        # Remove the prefix\n",
        "        new_filename = filename[len(prefix):]\n",
        "        old_filepath = os.path.join(wav_directory, filename)\n",
        "        new_filepath = os.path.join(wav_directory, new_filename)\n",
        "        print(f\"Renaming: {old_filepath} -> {new_filepath}\")\n",
        "        os.rename(old_filepath, new_filepath)\n",
        "\n",
        "print(\"Renaming training audio files complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjjLApbsvgLC"
      },
      "source": [
        "Creating Kaldi-style files: wav.scp, text, and utt2spk and converting Buckwalter to Standard Arabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrKWymI0_0bs",
        "outputId": "0bf01649-490a-43f6-99a3-15a4f649efd0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from camel_tools.utils.transliterate import Transliterator\n",
        "from camel_tools.utils.charmap import CharMapper\n",
        "\n",
        "# Initialize the Buckwalter transliterator\n",
        "bw2ar = CharMapper.builtin_mapper(\"bw2ar\")\n",
        "bt = Transliterator(bw2ar)\n",
        "\n",
        "# Set your dataset paths (update these paths as needed)\n",
        "dataset_path = '/content/arabic-speech-corpus'  # Replace with your dataset root directory\n",
        "wav_dir = os.path.join(dataset_path, 'wav')\n",
        "transcript_file = os.path.join(dataset_path, 'phonetic-transcipt.txt')\n",
        "\n",
        "# Output directory for Kaldi-style files (e.g., for training)\n",
        "kaldi_data_dir = '/content/arabic-speech-corpus/kaldi_data/train'  # Update this as needed\n",
        "os.makedirs(kaldi_data_dir, exist_ok=True)\n",
        "\n",
        "# Open output files for Kaldi-style directory\n",
        "wav_scp = open(os.path.join(kaldi_data_dir, 'wav.scp'), 'w', encoding='utf-8')\n",
        "text_f = open(os.path.join(kaldi_data_dir, 'text'), 'w', encoding='utf-8')\n",
        "utt2spk = open(os.path.join(kaldi_data_dir, 'utt2spk'), 'w', encoding='utf-8')\n",
        "\n",
        "# Set a default speaker ID (adjust if you have multiple speakers)\n",
        "default_spk = \"arabic\"\n",
        "\n",
        "with open(transcript_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        # Remove extra quotes and split the line into fields\n",
        "        # Expected format: \"ARA NORM  0002.wav\" \"buckwalter transcription\"\n",
        "        parts = line.strip().split('\" \"')\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "        # Clean up the fields (remove any remaining quotes)\n",
        "        utt_field = parts[0].replace('\"', '').strip()\n",
        "        buckwalter_transcription = parts[1].replace('\"', '').strip()\n",
        "\n",
        "        # Extract the filename from utt_field. Example: \"ARA NORM  0002.wav\"\n",
        "        utt_filename = utt_field.split()[-1]\n",
        "        # Remove the file extension to create an utterance ID (e.g., \"0002\")\n",
        "        utt_id = os.path.splitext(utt_filename)[0]\n",
        "\n",
        "        #Has been commented out to test with phonetic transcription\n",
        "\n",
        "        # Convert the Buckwalter transcription to standard Arabic script\n",
        "        # arabic_transcription = bt.transliterate(buckwalter_transcription)\n",
        "\n",
        "        # Write to wav.scp (assumes the wav files are in the wav/ directory)\n",
        "        wav_path = os.path.join(wav_dir, utt_filename)\n",
        "        wav_scp.write(f\"{utt_id} {wav_path}\\n\")\n",
        "\n",
        "        # Write the converted Arabic transcript to the text file\n",
        "        text_f.write(f\"{utt_id} {buckwalter_transcription}\\n\")\n",
        "\n",
        "        # Write to utt2spk (assign default speaker)\n",
        "        utt2spk.write(f\"{utt_id} {default_spk}\\n\")\n",
        "\n",
        "# Close the files\n",
        "wav_scp.close()\n",
        "text_f.close()\n",
        "utt2spk.close()\n",
        "\n",
        "print(\"Kaldi-style training data files have been created in:\", kaldi_data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV2O7yXvAQjB",
        "outputId": "8173d3dc-fbbe-4a2a-db35-8ac180266c43"
      },
      "outputs": [],
      "source": [
        "# Set your dataset paths (update these paths as needed)\n",
        "dataset_path = '/content/arabic-speech-corpus/test set'  # Replace with your dataset root directory\n",
        "wav_dir = os.path.join(dataset_path, 'wav')\n",
        "transcript_file = os.path.join(dataset_path, 'phonetic-transcript.txt')\n",
        "\n",
        "# Output directory for Kaldi-style files (e.g., for training)\n",
        "kaldi_data_dir = '/content/arabic-speech-corpus/kaldi_data/test'  # Update this as needed\n",
        "os.makedirs(kaldi_data_dir, exist_ok=True)\n",
        "\n",
        "# Open output files for Kaldi-style directory\n",
        "wav_scp = open(os.path.join(kaldi_data_dir, 'wav.scp'), 'w', encoding='utf-8')\n",
        "text_f = open(os.path.join(kaldi_data_dir, 'text'), 'w', encoding='utf-8')\n",
        "utt2spk = open(os.path.join(kaldi_data_dir, 'utt2spk'), 'w', encoding='utf-8')\n",
        "\n",
        "# Set a default speaker ID (adjust if you have multiple speakers)\n",
        "default_spk = \"arabic\"\n",
        "\n",
        "with open(transcript_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        # Remove extra quotes and split the line into fields\n",
        "        # Expected format: \"ARA NORM  0002.wav\" \"buckwalter transcription\"\n",
        "        parts = line.strip().split('\" \"')\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "        # Clean up the fields (remove any remaining quotes)\n",
        "        utt_field = parts[0].replace('\"', '').strip()\n",
        "        buckwalter_transcription = parts[1].replace('\"', '').strip()\n",
        "\n",
        "        # Extract the filename from utt_field. Example: \"ARA NORM  0002.wav\"\n",
        "        utt_filename = utt_field.split()[-1]\n",
        "        # Remove the file extension to create an utterance ID (e.g., \"0002\")\n",
        "        utt_id = os.path.splitext(utt_filename)[0]\n",
        "\n",
        "        #Has been commented out to test with phonetic transcription\n",
        "\n",
        "        # Convert the Buckwalter transcription to standard Arabic script\n",
        "        # arabic_transcription = bt.transliterate(buckwalter_transcription)\n",
        "\n",
        "        # Write to wav.scp (assumes the wav files are in the wav/ directory)\n",
        "        wav_path = os.path.join(wav_dir, utt_filename)\n",
        "        wav_scp.write(f\"{utt_id} {wav_path}\\n\")\n",
        "\n",
        "        # Write the converted Arabic transcript to the text file\n",
        "        text_f.write(f\"{utt_id} {buckwalter_transcription}\\n\")\n",
        "\n",
        "        # Write to utt2spk (assign default speaker)\n",
        "        utt2spk.write(f\"{utt_id} {default_spk}\\n\")\n",
        "\n",
        "# Close the files\n",
        "wav_scp.close()\n",
        "text_f.close()\n",
        "utt2spk.close()\n",
        "\n",
        "print(\"Kaldi-style testing data files have been created in:\", kaldi_data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qsmipz3Ckv8",
        "outputId": "b2266442-cfb0-4bc0-94ac-e559903dfa50"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the paths to your utt2spk and the output spk2utt file\n",
        "utt2spk_path = '/content/arabic-speech-corpus/kaldi_data/train/utt2spk'  # Update this path\n",
        "spk2utt_path = '/content/arabic-speech-corpus/kaldi_data/train/spk2utt'    # Update this path\n",
        "\n",
        "# Dictionary to accumulate utterances for each speaker\n",
        "speaker_dict = {}\n",
        "\n",
        "# Read utt2spk file\n",
        "with open(utt2spk_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) != 2:\n",
        "            continue  # Skip any malformed lines\n",
        "        utt, spk = parts\n",
        "        if spk not in speaker_dict:\n",
        "            speaker_dict[spk] = []\n",
        "        speaker_dict[spk].append(utt)\n",
        "\n",
        "# Write spk2utt file\n",
        "with open(spk2utt_path, 'w', encoding='utf-8') as f:\n",
        "    for spk, utt_list in speaker_dict.items():\n",
        "        f.write(f\"{spk} {' '.join(utt_list)}\\n\")\n",
        "\n",
        "print(f\"train spk2utt file has been created at: {spk2utt_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDmgUZYiCxKH",
        "outputId": "afc7557c-048a-4d51-b10b-bc9d3280da96"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the paths to your utt2spk and the output spk2utt file\n",
        "utt2spk_path = '/content/arabic-speech-corpus/kaldi_data/test/utt2spk'  # Update this path\n",
        "spk2utt_path = '/content/arabic-speech-corpus/kaldi_data/test/spk2utt'    # Update this path\n",
        "\n",
        "# Dictionary to accumulate utterances for each speaker\n",
        "speaker_dict = {}\n",
        "\n",
        "# Read utt2spk file\n",
        "with open(utt2spk_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) != 2:\n",
        "            continue  # Skip any malformed lines\n",
        "        utt, spk = parts\n",
        "        if spk not in speaker_dict:\n",
        "            speaker_dict[spk] = []\n",
        "        speaker_dict[spk].append(utt)\n",
        "\n",
        "# Write spk2utt file\n",
        "with open(spk2utt_path, 'w', encoding='utf-8') as f:\n",
        "    for spk, utt_list in speaker_dict.items():\n",
        "        f.write(f\"{spk} {' '.join(utt_list)}\\n\")\n",
        "\n",
        "print(f\"test spk2utt file has been created at: {spk2utt_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NioRq7yzwv5a"
      },
      "source": [
        "Creating Token list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c95P1LCfJmcI",
        "outputId": "e2ad173b-b4ed-41ea-c112-b763968a50ee"
      },
      "outputs": [],
      "source": [
        "kaldi_data_dir = \"/content/arabic-speech-corpus/kaldi_data/train\"\n",
        "\n",
        "token_set = set()\n",
        "text_path = os.path.join(kaldi_data_dir, 'text')\n",
        "with open(text_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split(maxsplit=1)\n",
        "        if len(parts) == 2:\n",
        "            transcript = parts[1]\n",
        "            token_set.update(list(transcript))\n",
        "\n",
        "# Write token list\n",
        "token_list_path = os.path.join(kaldi_data_dir, 'token_list.txt')\n",
        "with open(token_list_path, 'w', encoding='utf-8') as f:\n",
        "    for token in sorted(token_set):\n",
        "        f.write(token + \"\\n\")\n",
        "print(\"Token list created at:\", token_list_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQmY6twrw1YG"
      },
      "source": [
        "Need to install 1.26.4 version of Numpy as Trainer.train() requires Dtypes. To check if other numpy versions are suitable use the below code.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "print(np.__version__)\n",
        "print(hasattr(np, \"dtypes\"))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ2u9F3gSSA8"
      },
      "source": [
        "# Re-Installing Libraries and Model Weights for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2ma2HkQHYXE",
        "outputId": "1afc3899-3279-4801-ca60-15745c3c19da"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-frZNDjxmmh"
      },
      "source": [
        "Downloading the model checkpoint and training configuration file from HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TwYSLANJ5Xp",
        "outputId": "4c25ba9b-5c8f-4472-ecea-48f316d9e2ad"
      },
      "outputs": [],
      "source": [
        "from espnet_model_zoo.downloader import ModelDownloader\n",
        "d = ModelDownloader()  # <module_dir> is used as cachedir by default\n",
        "# model_id = \"espnet/kan-bayashi_libritts_xvector_vits\" #originally used to train VITS model\n",
        "model_id = \"kan-bayashi/ljspeech_tacotron2\"\n",
        "\n",
        "model_dir = d.download_and_unpack(model_id)\n",
        "print(f\"Model '{model_id}' downloaded and unpacked at: {model_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOIdbonVx5-T"
      },
      "source": [
        "Filling the variables DUMP_DIR and data_info for training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTSSRKpMPftc"
      },
      "outputs": [],
      "source": [
        "arabic_data_dir = \"/content/arabic-speech-corpus/kaldi_data\"\n",
        "# Directory containing your dumped Arabic dataset in Kaldi-style\n",
        "DUMP_DIR = arabic_data_dir\n",
        "\n",
        "# Data information mapping keys to file names and types:\n",
        "data_info = {\n",
        "    \"speech\": [\"wav.scp\", \"sound\"],\n",
        "    \"text\": [\"text\", \"text\"],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Hq-LJBnzE9n"
      },
      "source": [
        "Installing Protobuf with version 3.20.1 since trainer.collect_stats() requires it. The pip depency error for Google can be ignored as it does not affect the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "MToZjSaaSb9v",
        "outputId": "9faf9001-a18b-4798-9660-095c07216854"
      },
      "outputs": [],
      "source": [
        "!pip install protobuf==3.20.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfjzEvjlzkfH"
      },
      "source": [
        "Importing espnetez"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgLVI_aZR5YE",
        "outputId": "cad654f4-5b18-470b-cab4-09b6ccdef85a"
      },
      "outputs": [],
      "source": [
        "import espnetez as ez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FM_kJCbzoYp"
      },
      "source": [
        "Logging into Wandb for metric gathering. It will ask for API key from Wandb to store and display the metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJS25ngdgcwo",
        "outputId": "7f554272-8c3d-4d03-c2c1-0125c1ad23a0"
      },
      "outputs": [],
      "source": [
        "!wandb login --relogin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqxJJ9sNSGzQ"
      },
      "source": [
        "# Configuring for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaMn4OOuz3am"
      },
      "source": [
        "Configuring the training config downloaded from HuggingFace. Depending on the task, finetune_config[\"tts\"] accepts certain models. The models can be seen in the link below through the files. This error will be shown in the Trainer.train()\n",
        "\n",
        "Link: https://github.com/espnet/espnet/tree/master/espnet2/tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPXOQyA3Naim"
      },
      "outputs": [],
      "source": [
        "TASK = \"tts\" #Depending on the model the task changes Eg: VITS works only with gan_tts task\n",
        "\n",
        "pretrain_config = ez.config.from_yaml(TASK, model_dir[\"train_config\"])\n",
        "\n",
        "# Update the configuration with the downloaded model file path\n",
        "pretrain_config[\"model_file\"] = model_dir[\"model_file\"]\n",
        "\n",
        "# Modify configuration for fine-tuning\n",
        "finetune_config = pretrain_config.copy()\n",
        "finetune_config[\"tts\"] = \"tacotron2\" #Models that comply with the task\n",
        "finetune_config[\"batch_size\"] = 1\n",
        "finetune_config[\"num_workers\"] = 1\n",
        "finetune_config[\"max_epoch\"] = 100\n",
        "finetune_config[\"batch_bins\"] = 500000\n",
        "finetune_config[\"num_iters_per_epoch\"] = 2\n",
        "finetune_config[\"generator_first\"] = True\n",
        "finetune_config[\"use_wandb\"] = True\n",
        "finetune_config[\"wandb_project\"] = \"ESPnet Training\"\n",
        "finetune_config[\"wandb_name\"] = \"ESPnet Tacatron2 run 100 epochs\"\n",
        "\n",
        "# Disable distributed training\n",
        "finetune_config[\"distributed\"] = False\n",
        "finetune_config[\"multiprocessing_distributed\"] = False\n",
        "finetune_config[\"dist_world_size\"] = None\n",
        "finetune_config[\"dist_rank\"] = None\n",
        "finetune_config[\"local_rank\"] = None\n",
        "finetune_config[\"dist_master_addr\"] = None\n",
        "finetune_config[\"dist_master_port\"] = None\n",
        "finetune_config[\"dist_launcher\"] = None\n",
        "finetune_config[\"pretrain_path\"] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xK7wl9S00os"
      },
      "source": [
        "Contents of the configuration yaml file been dumped to verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OpzFO-1qBbd",
        "outputId": "0fc2aab5-1b27-4b42-b1c1-8e32b618a51f"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "print(\"Fine-tuning configuration:\")\n",
        "print(yaml.dump(finetune_config, sort_keys=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqre9G3k1Axo"
      },
      "source": [
        "Defining Experiment and Stats Directory, and initializes ez.Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-EcgOMmONKs"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = \"asc\"\n",
        "EXP_DIR = f\"./exp/finetune_{TASK}_{DATASET_NAME}\" ## output directory containing the trained model weights and config.yaml file\n",
        "STATS_DIR = f\"./exp/stats_{DATASET_NAME}\"\n",
        "ngpu = 1\n",
        "\n",
        "trainer = ez.Trainer(\n",
        "    task=TASK,\n",
        "    train_config=finetune_config,\n",
        "    train_dump_dir=f\"{DUMP_DIR}/train\",\n",
        "    valid_dump_dir=f\"{DUMP_DIR}/test\",\n",
        "    data_info=data_info,\n",
        "    output_dir=EXP_DIR,\n",
        "    stats_dir=STATS_DIR,\n",
        "    ngpu=ngpu,\n",
        ")\n",
        "\n",
        "# Add the xvector paths to the configuration\n",
        "trainer.train_config.train_data_path_and_name_and_type += [\n",
        "    [\"/content/train_speaker_embeddings/train_spk_embed.scp\", \"spembs\", \"kaldi_ark\"],\n",
        "]\n",
        "trainer.train_config.valid_data_path_and_name_and_type += [\n",
        "    [\"/content/test_speaker_embeddings/test_spk_embed.scp\", \"spembs\", \"kaldi_ark\"],\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2YM_Sko1bHB"
      },
      "source": [
        "Downloading NLTK POS Tagger. Even if the it is been downloded while importing espnetez, an error is thrown that averaged_perceptron_tagger_eng is missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ebFeO0KE3m7",
        "outputId": "9213a111-f04f-4e6a-d098-1019afe99bf1"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKuGNllvL-Vi"
      },
      "source": [
        "Collecting stats from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFKwUxJIRvmt",
        "outputId": "d1e72de2-ce6d-4dee-98d1-ba3d16552061"
      },
      "outputs": [],
      "source": [
        "# Temporarily disable normalization to collect stats\n",
        "trainer.train_config.normalize = None\n",
        "trainer.train_config.pitch_normalize = None\n",
        "trainer.train_config.energy_normalize = None\n",
        "\n",
        "# Collect statistics from the training dump\n",
        "trainer.collect_stats()\n",
        "\n",
        "# After collecting stats, re-enable normalization if required.\n",
        "trainer.train_config.write_collected_feats = False\n",
        "if finetune_config.get(\"normalize\") is not None:\n",
        "    trainer.train_config.normalize = finetune_config[\"normalize\"]\n",
        "    trainer.train_config.normalize_conf[\"stats_file\"] = f\"{STATS_DIR}/train/feats_stats.npz\"\n",
        "if finetune_config.get(\"pitch_normalize\") is not None:\n",
        "    trainer.train_config.pitch_normalize = finetune_config[\"pitch_normalize\"]\n",
        "    trainer.train_config.pitch_normalize_conf[\"stats_file\"] = f\"{STATS_DIR}/train/pitch_stats.npz\"\n",
        "if finetune_config.get(\"energy_normalize\") is not None:\n",
        "    trainer.train_config.energy_normalize = finetune_config[\"energy_normalize\"]\n",
        "    trainer.train_config.energy_normalize_conf[\"stats_file\"] = f\"{STATS_DIR}/train/energy_stats.npz\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4UmTtEZR8XS"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmSfCxLfMC-s"
      },
      "source": [
        "Training/Fine-tuning the model. The output of this training is in the exp folder, the exp folder will be need for inferencing.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9DXSHkUeR724",
        "outputId": "f9dda54f-4750-494b-e09d-a9f23388f6a2"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2dfP0FnR0EC"
      },
      "source": [
        "# Inferencing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fheAYNeGNHQ5"
      },
      "source": [
        "Inferencing the model using the fine-tuned model weights. Sometimes other models only require config.yaml file but others will throw an error if the exp folder is missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avYSsq08M4qm"
      },
      "outputs": [],
      "source": [
        "from espnet2.bin.tts_inference import Text2Speech\n",
        "import kaldiio  # This is commonly used to read Kaldi-style scp files\n",
        "sf.write(\"output.wav\", wav.numpy(), tts.fs, \"PCM_16\")\n",
        "\n",
        "# The scp file is just a mapping file - you need to get an actual embedding\n",
        "# First, load the mapping\n",
        "spk_dict = kaldiio.load_scp(\"/content/train_speaker_embeddings/train_spk_embed.scp\")\n",
        "\n",
        "# Get the first speaker embedding\n",
        "spk_id = list(spk_dict.keys())[0]  # Get the first speaker ID\n",
        "spembs = spk_dict[spk_id]  # Get the embedding for that speaker\n",
        "\n",
        "# with local model\n",
        "tts = Text2Speech.from_pretrained(model_file=\"/content/67epoch.pth\")\n",
        "wav = tts(\"sil w a r a' jj A H a tt A q r ii0' r u0 ll a * i0 < a E a' dd a h u0 m a' E h a d u0 < a b H aa' ^ i0 h A D A' b a t i0 tt i1' b t i0 f i0 l < a k aa d ii0 m ii0' y a t i0 SS II0 n ii0' y a t i0 l u0 l E u0 l uu0' m i0 sil < a' n t a s t a m i0' rr a d a r a j aa' t u0 l H a r aa' r a t i0 w a m u0 s t a w a y aa' t u0 rr U0 T UU0' b a t i0 f i0 l Ah i0 r t i0 f aa' E i0 T A' w A l a h aa' * a l q A' r n sil\",spembs=spembs)[\"wav\"]\n",
        "sf.write(\"output.wav\", wav.numpy(), tts.fs, \"PCM_16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yag5BNzRR4Tf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
